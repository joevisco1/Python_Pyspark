{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPIBbQcQO2HCdFt/4Q3xEtj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Nnz5HR6m8N5c"},"outputs":[],"source":["#Spark achieves simplicity by providing a fundamental abstraction of a simple logical data structure called a Resilient Distributed Dataset (RDD)\n","#upon which all other higher-level structured data abstractions, such as DataFrames and Datasets, are constructed.\n","#Unlike Apache Hadoop, which included both storage and compute, Spark decouples the two.\n","\n","# Spark SQL, Spark MLlib, Spark Structured Streaming, and GraphX\n"]},{"cell_type":"markdown","source":["##Spark Components"],"metadata":{"id":"JFFEXAxo_mRT"}},{"cell_type":"markdown","source":["### Spark SQL"],"metadata":{"id":"Of2SnY2q-lZU"}},{"cell_type":"code","source":["\n","## Read data off Amazon S3 bucket into a Spark DataFrame\n","spark.read.json(\"s3://apache_spark/data/committers.json\")\n","  .createOrReplaceTempView(\"committers\")\n","## Issue a SQL query and return the result as a Spark DataFrame\n","val results = spark.sql(\"\"\"SELECT name, org, module, release, num_commits\n","    FROM committers WHERE module = 'mllib' AND num_commits > 10\n","    ORDER BY num_commits DESC\"\"\")"],"metadata":{"id":"K7VwbIcf9a59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Spark ML"],"metadata":{"id":"2obAMCiw-x4T"}},{"cell_type":"code","source":["##Spark comes with a library containing common machine learning (ML) algorithms called MLlib\n","#spark.mllib - RDD Based\n","#spark.ml - Dataframe Based"],"metadata":{"id":"kCZjHi859klc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build Model Spark In Python\n","from pyspark.ml.classification import LogisticRegression\n","...\n","training = spark.read.csv(\"s3://...\")\n","test = spark.read.csv(\"s3://...\")\n","\n","# Load training data\n","lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n","\n","# Fit the model\n","lrModel = lr.fit(training)\n","\n","# Predict\n","lrModel.transform(test)"],"metadata":{"id":"5Q6IKfM49mds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Spark Structured Streaming"],"metadata":{"id":"j_b3_ss2-01L"}},{"cell_type":"code","source":["## Get Wordcount\n","# In Python\n","# Read a stream from a local host\n","from pyspark.sql.functions import explode, split\n","lines = (spark\n","  .readStream\n","  .format(\"socket\")\n","  .option(\"host\", \"localhost\")\n","  .option(\"port\", 9999)\n","  .load())\n","\n","# Perform transformation\n","# Split the lines into words\n","words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n","\n","# Generate running word count\n","word_counts = words.groupBy(\"word\").count()\n","\n","# Write out to the stream to Kafka\n","query = (word_counts\n","  .writeStream\n","  .format(\"kafka\")\n","  .option(\"topic\", \"output\"))"],"metadata":{"id":"AVNBMIt2-Y38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GraphX"],"metadata":{"id":"VgMWnMaI_FkD"}},{"cell_type":"code","source":["## In Scala\n","val graph = Graph(vertices, edges)\n","messages = spark.textFile(\"hdfs://...\")\n","val graph2 = graph.joinVertices(messages) {\n","  (id, vertex, msg) => ...\n","}"],"metadata":{"id":"_vxTdlMB_A7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spark Sessions"],"metadata":{"id":"mRTJlxZuAeNU"}},{"cell_type":"code","source":["\n","# In Spark 2.0, the SparkSession became a unified conduit to all Spark operations and data. Not only did it subsume previous entry points to Spark like the SparkContext, SQLContext, HiveContext,\n","# SparkConf, and StreamingContext, but it also made working with Spark simpler and easier."],"metadata":{"id":"4vdr6S92_wGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vXZTij4jAcvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In Scala\n","import org.apache.spark.sql.SparkSession\n","\n","# Build SparkSession\n","val spark = SparkSession\n","  .builder\n","  .appName(\"LearnSpark\")\n","  .config(\"spark.sql.shuffle.partitions\", 6)\n","  .getOrCreate()\n","...\n","# Use the session to read JSON\n","val people = spark.read.json(\"...\")\n","...\n","# Use the session to issue a SQL query\n","val resultsDF = spark.sql(\"SELECT city, pop, state, zip FROM table_name\")"],"metadata":{"id":"l0X6uUpRAPT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Currently, Spark supports four cluster managers: the built-in standalone cluster manager, Apache Hadoop YARN, Apache Mesos, and Kubernetes.\n","\n","#Mode\tSpark driver\tSpark executor\tCluster manager\n","#Local\t - Runs on a single JVM, like a laptop or single node\tRuns on the same JVM as the driver\tRuns on the same host\n","#Standalone\tCan run on any node in the cluster\tEach node in the cluster will launch its own executor JVM\tCan be allocated arbitrarily to any host in the cluster\n","#YARN (client)\tRuns on a client, not part of the cluster\tYARN’s NodeManager’s container\tYARN’s Resource Manager works with YARN’s Application Master to allocate the containers on NodeManagers for executors\n","#YARN (cluster)\tRuns with the YARN Application Master\tSame as YARN client mode\tSame as YARN client mode\n","#Kubernetes\tRuns in a Kubernetes pod\tEach worker runs within its own pod\tKubernetes Master"],"metadata":{"id":"AvjvvRpMAXdk"},"execution_count":null,"outputs":[]}]}